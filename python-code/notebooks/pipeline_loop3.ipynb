{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:12.354210Z",
     "start_time": "2023-09-07T10:53:12.344883Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import torch\n",
    "import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:18.715156Z",
     "start_time": "2023-09-07T10:53:18.673755Z"
    }
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'validate_every': 10,\n",
    "    'batch_size': 128,\n",
    "    'stopper_patience': 50, # should be at multiple of validate_every (the min is taken from stopper_patience / validate_every # loss samples.\n",
    "    'stopper_delta': 0,\n",
    "    'stopper_min_epochs': 20,\n",
    "    'epoch': 10,\n",
    "    'savefig': True,\n",
    "    'net_logging_level': logging.INFO\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running instructions\n",
    "0. Go to dir:\n",
    "```cd com1-github/python/notebooks```\n",
    "1. Convert to .py using:\n",
    "```jupyter nbconvert pipeline_loop3.ipynb --to python```\n",
    "2. Remove logs / images from outer_dir (com1-github/logs/torch_cv/)\n",
    "3. Run the script:\n",
    "```./pipeline_loop3.sh```\n",
    "I needed to encapsulate the script in a sh-script, that restarts the script every ten iterations. To counter a mem leak. Be sure to give the bash-script chmod +x rights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "rng = np.random.RandomState(313)\n",
    "\n",
    "data_dir = \"data/feature_sets/\"\n",
    "output_dir = \"logs/torch_cv_step_4/\"\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(313)\n",
    "print(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:22.194121Z",
     "start_time": "2023-09-07T10:53:22.154551Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Used to build a log for the individual grid rows / classification attemps\n",
    "def new_log(name, filename):\n",
    "    log = logging.getLogger(name)\n",
    "    for handler in log.handlers[:]:\n",
    "        log.removeHandler(handler)\n",
    "    #log.setLevel(logging.WARNING)\n",
    "    format = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    file_handler = logging.FileHandler(output_dir + filename,mode='w') # w means overwrite?\n",
    "    file_handler.setFormatter(format)\n",
    "    log.addHandler(file_handler)\n",
    "    log.setLevel(PARAMS['net_logging_level'])\n",
    "    log.propagate = False\n",
    "    return log\n",
    "\n",
    "log = new_log(\"test\", \"test.log\")\n",
    "log.setLevel(logging.INFO)\n",
    "log.info(\"test\")\n",
    "#log.shutdown()\n",
    "log.handlers.clear()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:23.076114Z",
     "start_time": "2023-09-07T10:53:22.935647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2023-09-07 12:53:23,875 - INFO - test info\n"
     ]
    }
   ],
   "source": [
    "# Setting up a root logger for jupyter (otherwise it doesn't work)\n",
    "# From https://stackoverflow.com/questions/54246623/how-to-toggle-jupyter-notebook-display-logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(format=' %(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"test info\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:23.840657Z",
     "start_time": "2023-09-07T10:53:23.823403Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers.load_parquet_data import load_parquet_data\n",
    "data_file = \"wundt_v16_8-model-ridge-na.parquet\"\n",
    "X, y, z, X_guardian, y_guardian,features = load_parquet_data(data_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([49075, 25])"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "X_ts = torch.from_numpy(X.to_numpy(np.float32)).to(device)\n",
    "y_ts = torch.from_numpy(y.to_numpy(np.int64)).to(device)\n",
    "X_guardian_ts = torch.from_numpy(X_guardian.to_numpy(np.float32)).to(device)\n",
    "X_ts.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:25.797339Z",
     "start_time": "2023-09-07T10:53:25.676276Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled X:\n",
      "tensor([[5, 6],\n",
      "        [1, 2],\n",
      "        [7, 8],\n",
      "        [3, 4]])\n",
      "Sampled y:\n",
      "tensor([0, 0, 1, 1])\n",
      "Sampled idxs:\n",
      "tensor([2, 0, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def random_undersample_t(Xt, yt, z=None):\n",
    "    class_counts = torch.bincount(yt)\n",
    "    min_class_count = torch.min(class_counts)\n",
    "\n",
    "    sampled_indices = []\n",
    "    for class_label in torch.unique(yt):\n",
    "        indices = torch.where(yt == class_label)[0]\n",
    "        sampled_indices.append(indices[torch.randperm(len(indices))[:min_class_count]])\n",
    "\n",
    "    sampled_indices = torch.cat(sampled_indices)\n",
    "    sampled_X = Xt[sampled_indices]\n",
    "    sampled_y = yt[sampled_indices]\n",
    "    sampled_z = None\n",
    "    if not z is None:\n",
    "        sampled_z = z.iloc[sampled_indices.detach().cpu().numpy()]\n",
    "\n",
    "    return sampled_X, sampled_y, sampled_z, sampled_indices\n",
    "\n",
    "# Example usage:\n",
    "X_artificial = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "y_artificial = torch.tensor([0, 1, 0, 1, 0])\n",
    "\n",
    "sampled_X, sampled_y, sampled_z, sampled_indices = random_undersample_t(X_artificial, y_artificial)\n",
    "\n",
    "print(\"Sampled X:\")\n",
    "print(sampled_X)\n",
    "\n",
    "print(\"Sampled y:\")\n",
    "print(sampled_y)\n",
    "\n",
    "print(\"Sampled idxs:\")\n",
    "print(sampled_indices)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:27.066993Z",
     "start_time": "2023-09-07T10:53:27.039208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "\n",
    "def standardize_t(tensor, mean=None, std=None):\n",
    "    if(mean==None): mean = tensor.mean(dim=0)\n",
    "    if(std==None): std = tensor.std(dim=0)\n",
    "    standardized_tensor = (tensor - mean) / std\n",
    "    return standardized_tensor, mean, std\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:28.249393Z",
     "start_time": "2023-09-07T10:53:28.225373Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def confusion_matrix_scorer(y, y_pred):\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1], 'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'classif.acc': accuracy_score(y_true,y_pred),\n",
    "        'classif.precision': precision_score(y_true,y_pred),\n",
    "        'classif.recall': recall_score(y_true,y_pred),\n",
    "        'classif.f1': f1_score(y_true,y_pred),\n",
    "        'classif.tn': confusion_matrix_scorer(y_true,y_pred)['tn'],\n",
    "        'classif.fp': confusion_matrix_scorer(y_true,y_pred)['fp'],\n",
    "        'classif.fn': confusion_matrix_scorer(y_true,y_pred)['fn'],\n",
    "        'classif.tp': confusion_matrix_scorer(y_true,y_pred)['tp'],\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:29.129477Z",
     "start_time": "2023-09-07T10:53:29.109410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2023-09-07 12:53:30,116 - INFO - Saving checkpoints at /tmp/tmpjlf80lhk\n"
     ]
    },
    {
     "data": {
      "text/plain": "<com1py.net.LogitNet at 0x7f81e96a23d0>"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from helpers.early_stopper import EarlyStopper\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "from com1py.net import LogitNet\n",
    "model = LogisticRegression(25).to(device)\n",
    "net = LogitNet(\n",
    "    model=model,\n",
    "    criterion=torch.nn.BCEWithLogitsLoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=.001),\n",
    "    scheduler=None,\n",
    "    stopper=EarlyStopper(patience=10, min_delta=.0001),\n",
    "    validate_every=10,\n",
    "    batch_size=128\n",
    ")\n",
    "net"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:30.107854Z",
     "start_time": "2023-09-07T10:53:30.049999Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "data_files = list()\n",
    "for file in os.listdir(data_dir):\n",
    "    # check only text files\n",
    "    if file.endswith('.parquet') and not \"combi\" in file:\n",
    "        data_files.append(file)\n",
    "#data_files = [\"wundt_v16_8-model-ridge-na.parquet\"]\n",
    "\n",
    "data_files = pd.Series(data_files)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:31.573145Z",
     "start_time": "2023-09-07T10:53:31.545202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers.load_parquet_data import load_parquet_data\n",
    "data_file = \"wundt_v16_8-model-ridge-na.parquet\"\n",
    "X, y, z, X_guardian, y_guardian,features = load_parquet_data(data_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:38.457531Z",
     "start_time": "2023-09-07T10:53:34.941749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2023-09-07 12:53:35,416 - INFO - Epoch 0/10, train loss: 0.7064, validation loss: 0.6920\n",
      " 2023-09-07 12:53:35,417 - DEBUG - current epoch: 0; current index: 0\n",
      " 2023-09-07 12:53:38,154 - INFO - Epoch 9/10, train loss: 0.6101, validation loss: 0.6081\n",
      " 2023-09-07 12:53:38,159 - DEBUG - saved checkpoints: []\n",
      " 2023-09-07 12:53:38,160 - DEBUG - checkpoints_with_losses: [(0.6081045866012573, 9)]\n",
      " 2023-09-07 12:53:38,160 - INFO - best epoch in window with checkpoint: (0.6081045866012573, 9)\n",
      " 2023-09-07 12:53:38,161 - INFO - Stop at None; best_epoch: 9; last_epoch: 9; num epochs: 10\n",
      " 2023-09-07 12:53:38,162 - DEBUG - Removing checkpoints from /tmp/tmpjlf80lhk\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# train-test split\n",
    "outer_cv = GroupShuffleSplit(n_splits=1, random_state=rng, test_size=None, train_size=0.8)\n",
    "\n",
    "# Manual loop\n",
    "for train_idxs, test_idxs in outer_cv.split(X_ts, y_ts, groups=z):\n",
    "    X_train, y_train = X_ts[train_idxs], y_ts[train_idxs]\n",
    "    X_test, y_test = X_ts[test_idxs], y_ts[test_idxs]\n",
    "\n",
    "    start_time = time.time()\n",
    "    net.train(PARAMS[\"epoch\"], X_train, y_train.float(), X_test, y_test.float())\n",
    "    log.info(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    logit, prob, pred = net.predict(X_test)\n",
    "    m = metrics(y_test.detach().cpu(), pred.detach().cpu())\n",
    "    log.info(\"metrics:%s\" % m)\n",
    "\n",
    "    # Plot the training loss\n",
    "    fig = net.plot_losses()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:38.581145Z",
     "start_time": "2023-09-07T10:53:38.457856Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2023-09-07 12:53:38,501 - INFO - probs: [0.5249037  0.49455288 0.3818153  0.6115081  0.41272625 0.33890152\n",
      " 0.50231856 0.37146714 0.38736022 0.47209924 0.41815737 0.34803277\n",
      " 0.31160554 0.2051718  0.25343022 0.3732     0.44219318 0.46274287]\n",
      " 2023-09-07 12:53:38,502 - INFO - logits: [ 0.09969735 -0.02178931 -0.48185027  0.45365572 -0.35270667 -0.6681932\n",
      "  0.00927436 -0.5259278  -0.45842206 -0.11171901 -0.3303423  -0.6276977\n",
      " -0.792624   -1.3542783  -1.0804005  -0.5185131  -0.23226595 -0.14930522]\n",
      " 2023-09-07 12:53:38,503 - INFO - stimuli: [ 9977.   322.  4584.  3819. 13165. 11143.  7784.  5938.  4094.  3775.\n",
      "  6474.  2725.  6046.  4701. 11299. 10879.  6366.  4504.]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'guardian.rho_noco2': 0.6174497467129644,\n 'guardian.rlogit_noco2': 0.6918631948085859,\n 'guardian.rprob_noco2': 0.6921642026820268,\n 'guardian.rho_comp': -0.5245225273080032,\n 'guardian.rlogit_comp': -0.5747220905549267,\n 'guardian.rprob_comp': -0.5789194686145116,\n 'guardian.rho_pf': -0.6264189886480908,\n 'guardian.rlogit_pf': -0.6492013661265331,\n 'guardian.rprob_pf': -0.6514979125440404}"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import json\n",
    "\n",
    "def guardian_eval(net, X_guardian_ts, guardian_df, log):\n",
    "    logits_t, prob_t, pred = net.predict(X_guardian_ts)\n",
    "    logits = logits_t.squeeze(1).detach().cpu().numpy()\n",
    "    prob = prob_t.squeeze(1).detach().cpu().numpy()\n",
    "    log.info(\"probs: %s\" % prob)\n",
    "    log.info(\"logits: %s\" % logits)\n",
    "    log.info(\"stimuli: %s\" % guardian_df.index.values)\n",
    "    res = {}\n",
    "    for var in ['noco2', 'comp', 'pf']:\n",
    "        rho, rho_p = spearmanr(guardian_df[var], logits)\n",
    "        r_logit = np.corrcoef(guardian_df[var], logits)[0, 1]\n",
    "        r_prob = np.corrcoef(guardian_df[var], prob)[0, 1]\n",
    "        res['guardian.rho_' + var] = rho\n",
    "        res['guardian.rlogit_' + var] = r_logit\n",
    "        res['guardian.rprob_' + var] = r_prob\n",
    "\n",
    "    #res['logit_json'] = json.dumps(logits.flatten().tolist())\n",
    "    #res['prob_json'] = json.dumps(prob.flatten().tolist())\n",
    "    return res\n",
    "\n",
    "# Train/test performance\n",
    "guardian_eval(net, X_guardian_ts, y_guardian, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid\n",
    "- The net__ part of the param names gets separated in the inner loop\n",
    "- List values are passed onto the inner loop for tuning\n",
    "- weight_decay controls the weight assigned to an L2 regularlization term (Ridge!). A range of 1e-4 to 1e-1 is recommended. GlmNET uses as minimum .0001 (1e-4). In my expanded grid cv script, I adjust this based on the depth (to account for increased #features), in steps .001 (feature ^ 2) and .01 (for feature ^3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:40.507976Z",
     "start_time": "2023-09-07T10:53:40.434394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                             data_file  outer_epoch  inner_epoch  net__depth  \\\n0   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n1   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n2   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n3   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n4   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n5   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n6   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n7   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n8   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n9   wundt_v16_8-model-ridge-na.parquet         1000          100           1   \n10  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n11  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n12  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n13  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n14  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n15  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n16  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n17  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n18  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n19  wundt_v16_8-model-ridge-na.parquet         1000          100           2   \n20  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n21  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n22  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n23  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n24  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n25  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n26  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n27  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n28  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n29  wundt_v16_8-model-ridge-na.parquet         1000          100           3   \n\n    net__width                           net__activation  \\\n0           16  [sigmoid, identity, tanh, relu, softmax]   \n1           32  [sigmoid, identity, tanh, relu, softmax]   \n2           48  [sigmoid, identity, tanh, relu, softmax]   \n3           64  [sigmoid, identity, tanh, relu, softmax]   \n4           80  [sigmoid, identity, tanh, relu, softmax]   \n5           96  [sigmoid, identity, tanh, relu, softmax]   \n6          112  [sigmoid, identity, tanh, relu, softmax]   \n7          128  [sigmoid, identity, tanh, relu, softmax]   \n8          144  [sigmoid, identity, tanh, relu, softmax]   \n9          160  [sigmoid, identity, tanh, relu, softmax]   \n10          16  [sigmoid, identity, tanh, relu, softmax]   \n11          32  [sigmoid, identity, tanh, relu, softmax]   \n12          48  [sigmoid, identity, tanh, relu, softmax]   \n13          64  [sigmoid, identity, tanh, relu, softmax]   \n14          80  [sigmoid, identity, tanh, relu, softmax]   \n15          96  [sigmoid, identity, tanh, relu, softmax]   \n16         112  [sigmoid, identity, tanh, relu, softmax]   \n17         128  [sigmoid, identity, tanh, relu, softmax]   \n18         144  [sigmoid, identity, tanh, relu, softmax]   \n19         160  [sigmoid, identity, tanh, relu, softmax]   \n20          16  [sigmoid, identity, tanh, relu, softmax]   \n21          32  [sigmoid, identity, tanh, relu, softmax]   \n22          48  [sigmoid, identity, tanh, relu, softmax]   \n23          64  [sigmoid, identity, tanh, relu, softmax]   \n24          80  [sigmoid, identity, tanh, relu, softmax]   \n25          96  [sigmoid, identity, tanh, relu, softmax]   \n26         112  [sigmoid, identity, tanh, relu, softmax]   \n27         128  [sigmoid, identity, tanh, relu, softmax]   \n28         144  [sigmoid, identity, tanh, relu, softmax]   \n29         160  [sigmoid, identity, tanh, relu, softmax]   \n\n               net__dropout                       net__lr_rate  \\\n0   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n1   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n2   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n3   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n4   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n5   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n6   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n7   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n8   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n9   [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n10  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n11  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n12  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n13  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n14  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n15  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n16  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n17  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n18  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n19  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n20  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n21  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n22  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n23  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n24  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n25  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n26  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n27  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n28  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n29  [0, 0.1, 0.2, 0.3, 0.4]  [1e-05, 0.0001, 0.001, 0.01, 0.1]   \n\n             net__weight_decay  \n0   [0.0001, 0.001, 0.01, 0.1]  \n1   [0.0001, 0.001, 0.01, 0.1]  \n2   [0.0001, 0.001, 0.01, 0.1]  \n3   [0.0001, 0.001, 0.01, 0.1]  \n4   [0.0001, 0.001, 0.01, 0.1]  \n5   [0.0001, 0.001, 0.01, 0.1]  \n6   [0.0001, 0.001, 0.01, 0.1]  \n7   [0.0001, 0.001, 0.01, 0.1]  \n8   [0.0001, 0.001, 0.01, 0.1]  \n9   [0.0001, 0.001, 0.01, 0.1]  \n10  [0.0001, 0.001, 0.01, 0.1]  \n11  [0.0001, 0.001, 0.01, 0.1]  \n12  [0.0001, 0.001, 0.01, 0.1]  \n13  [0.0001, 0.001, 0.01, 0.1]  \n14  [0.0001, 0.001, 0.01, 0.1]  \n15  [0.0001, 0.001, 0.01, 0.1]  \n16  [0.0001, 0.001, 0.01, 0.1]  \n17  [0.0001, 0.001, 0.01, 0.1]  \n18  [0.0001, 0.001, 0.01, 0.1]  \n19  [0.0001, 0.001, 0.01, 0.1]  \n20  [0.0001, 0.001, 0.01, 0.1]  \n21  [0.0001, 0.001, 0.01, 0.1]  \n22  [0.0001, 0.001, 0.01, 0.1]  \n23  [0.0001, 0.001, 0.01, 0.1]  \n24  [0.0001, 0.001, 0.01, 0.1]  \n25  [0.0001, 0.001, 0.01, 0.1]  \n26  [0.0001, 0.001, 0.01, 0.1]  \n27  [0.0001, 0.001, 0.01, 0.1]  \n28  [0.0001, 0.001, 0.01, 0.1]  \n29  [0.0001, 0.001, 0.01, 0.1]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data_file</th>\n      <th>outer_epoch</th>\n      <th>inner_epoch</th>\n      <th>net__depth</th>\n      <th>net__width</th>\n      <th>net__activation</th>\n      <th>net__dropout</th>\n      <th>net__lr_rate</th>\n      <th>net__weight_decay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>16</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>32</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>48</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>64</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>80</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>96</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>112</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>128</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>144</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>160</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>16</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>32</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>48</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>64</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>80</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>96</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>112</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>128</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>144</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>160</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>16</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>32</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>48</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>64</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>80</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>96</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>112</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>128</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>144</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>wundt_v16_8-model-ridge-na.parquet</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>160</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "def expand_grid(dictionary):\n",
    "   return pd.DataFrame([row for row in product(*dictionary.values())],\n",
    "                       columns=dictionary.keys())\n",
    "\n",
    "wundt_files = data_files[data_files.str.contains('wundt')]\n",
    "wundt_features = 25\n",
    "wundt_grid = expand_grid({\n",
    "    'data_file': wundt_files,\n",
    "    'outer_epoch': [1000], # 100, 1000\n",
    "    'inner_epoch': [100], # 100\n",
    "              'net__depth': [1,2,3],\n",
    "              'net__width': list(range(4, 164+1, 4)), # 40 values,\n",
    "              'net__activation': [['sigmoid', 'tanh', 'relu', 'softmax']], # 'identity',\n",
    "              'net__dropout': [[0, .1,.2,.3,.4]],\n",
    "              #'net__module__activation': ['sigmoid', 'identity', 'tanh'],\n",
    "              'net__lr_rate': [[.00001, .0001, .001, .01, .1]], # higher values (0.2, 0.3) don't work\n",
    "        'net__weight_decay': [[.0001, .001, .01, .1]] # Recommended 1e-4 to 1e-1. GlmNET uses as minimum .0001. Shouldn't be too low, as we want some level of regularlization.\n",
    "})\n",
    "\n",
    "# default function of a logistic model is sigmoid (for the output)\n",
    "# width is not used for a model that only contains an output layer\n",
    "wundt_grid = wundt_grid.loc[wundt_grid.astype(str).drop_duplicates().index].reset_index(drop=True) # drop drops the old index\n",
    "wundt_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            data_file  outer_epoch  \\\n0   bert_base_uncased-features_embedding-layer-10_...         1000   \n1   bert_base_uncased-features_embedding-layer-10_...         1000   \n2   bert_base_uncased-features_embedding-layer-10_...         1000   \n3   bert_base_uncased-features_embedding-layer-10_...         1000   \n4   bert_base_uncased-features_embedding-layer-10_...         1000   \n5   bert_base_uncased-features_embedding-layer-10_...         1000   \n6   bert_base_uncased-features_embedding-layer-10_...         1000   \n7   bert_base_uncased-features_embedding-layer-10_...         1000   \n8   bert_base_uncased-features_embedding-layer-10_...         1000   \n9   bert_base_uncased-features_embedding-layer-10_...         1000   \n10  bert_base_uncased-features_embedding-layer-10_...         1000   \n11  bert_base_uncased-features_embedding-layer-10_...         1000   \n12  bert_base_uncased-features_embedding-layer-10_...         1000   \n13  bert_base_uncased-features_embedding-layer-10_...         1000   \n14  bert_base_uncased-features_embedding-layer-10_...         1000   \n15  bert_base_uncased-features_embedding-layer-10_...         1000   \n16  bert_base_uncased-features_embedding-layer-10_...         1000   \n17  bert_base_uncased-features_embedding-layer-10_...         1000   \n18  bert_base_uncased-features_embedding-layer-10_...         1000   \n19  bert_base_uncased-features_embedding-layer-10_...         1000   \n20  bert_base_uncased-features_embedding-layer-10_...         1000   \n21  bert_base_uncased-features_embedding-layer-10_...         1000   \n22  bert_base_uncased-features_embedding-layer-10_...         1000   \n23  bert_base_uncased-features_embedding-layer-10_...         1000   \n24  bert_base_uncased-features_embedding-layer-10_...         1000   \n25  bert_base_uncased-features_embedding-layer-10_...         1000   \n26  bert_base_uncased-features_embedding-layer-10_...         1000   \n27  bert_base_uncased-features_embedding-layer-10_...         1000   \n28  bert_base_uncased-features_embedding-layer-10_...         1000   \n29  bert_base_uncased-features_embedding-layer-10_...         1000   \n\n    inner_epoch  net__depth  net__width  \\\n0           100           1          16   \n1           100           1          32   \n2           100           1          48   \n3           100           1          64   \n4           100           1          80   \n5           100           1          96   \n6           100           1         112   \n7           100           1         128   \n8           100           1         144   \n9           100           1         160   \n10          100           2          16   \n11          100           2          32   \n12          100           2          48   \n13          100           2          64   \n14          100           2          80   \n15          100           2          96   \n16          100           2         112   \n17          100           2         128   \n18          100           2         144   \n19          100           2         160   \n20          100           3          16   \n21          100           3          32   \n22          100           3          48   \n23          100           3          64   \n24          100           3          80   \n25          100           3          96   \n26          100           3         112   \n27          100           3         128   \n28          100           3         144   \n29          100           3         160   \n\n                             net__activation             net__dropout  \\\n0   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n1   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n2   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n3   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n4   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n5   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n6   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n7   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n8   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n9   [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n10  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n11  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n12  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n13  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n14  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n15  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n16  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n17  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n18  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n19  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n20  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n21  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n22  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n23  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n24  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n25  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n26  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n27  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n28  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n29  [sigmoid, identity, tanh, relu, softmax]  [0, 0.1, 0.2, 0.3, 0.4]   \n\n                         net__lr_rate           net__weight_decay  \n0   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n1   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n2   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n3   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n4   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n5   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n6   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n7   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n8   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n9   [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n10  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n11  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n12  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n13  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n14  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n15  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n16  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n17  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n18  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n19  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n20  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n21  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n22  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n23  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n24  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n25  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n26  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n27  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n28  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  \n29  [1e-05, 0.0001, 0.001, 0.01, 0.1]  [0.0001, 0.001, 0.01, 0.1]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data_file</th>\n      <th>outer_epoch</th>\n      <th>inner_epoch</th>\n      <th>net__depth</th>\n      <th>net__width</th>\n      <th>net__activation</th>\n      <th>net__dropout</th>\n      <th>net__lr_rate</th>\n      <th>net__weight_decay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>16</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>32</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>48</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>64</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>80</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>96</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>112</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>128</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>144</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>1</td>\n      <td>160</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>16</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>32</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>48</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>64</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>80</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>96</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>112</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>128</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>144</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>2</td>\n      <td>160</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>16</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>32</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>48</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>64</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>80</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>96</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>112</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>128</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>144</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>bert_base_uncased-features_embedding-layer-10_...</td>\n      <td>1000</td>\n      <td>100</td>\n      <td>3</td>\n      <td>160</td>\n      <td>[sigmoid, identity, tanh, relu, softmax]</td>\n      <td>[0, 0.1, 0.2, 0.3, 0.4]</td>\n      <td>[1e-05, 0.0001, 0.001, 0.01, 0.1]</td>\n      <td>[0.0001, 0.001, 0.01, 0.1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_files = data_files[data_files.str.contains('wundt')==False]\n",
    "bert_features = 768\n",
    "bert_grid = expand_grid({\n",
    "    'data_file': bert_files,\n",
    "    'outer_epoch': [1000], # 100, 1000\n",
    "    'inner_epoch': [100], # 100\n",
    "              'net__depth': [1,2,3],\n",
    "              'net__width': list(range(4, 164+1, 4)), # 40 values\n",
    "              'net__activation': [['sigmoid', 'tanh', 'relu', 'softmax']], # 'identity',\n",
    "              'net__dropout': [[0, .1,.2,.3,.4]],\n",
    "              'net__lr_rate': [[.00001, .0001, .001, .01, .1]], # higher values (0.2, 0.3) don't work\n",
    "        'net__weight_decay': [[.0001, .001, .01, .1]] # Recommended 1e-4 to 1e-1. GlmNET uses as minimum .0001\n",
    "})\n",
    "bert_grid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:53:42.992350Z",
     "start_time": "2023-09-07T10:53:42.935369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "(84, 9)"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_grid = pd.concat([wundt_grid,bert_grid])\n",
    "outer_grid.loc[outer_grid['net__depth']==1, ['net__width', 'net__dropout', 'net__activation']] = None\n",
    "outer_grid = outer_grid.loc[outer_grid.astype(str).drop_duplicates(keep='first').index]\n",
    "outer_grid.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T11:00:12.384458Z",
     "start_time": "2023-09-07T11:00:12.341143Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "row = outer_grid.iloc[:1,:]\n",
    "column_names = \"(`\" + \"`,`\".join(row.keys()) + \"`)\"\n",
    "values = \"('\" + \"','\".join(row.astype(str)) + \"')\"\n",
    "query = f\"INSERT INTO xxx {column_names} VALUES {values}\"\n",
    "\n",
    "import math\n",
    "\n",
    "row = outer_grid.iloc[0,:].to_dict()\n",
    "values = [str(v) for v in row.values()]\n",
    "values = [ \"NULL\" if v.lower() == \"nan\" or v.lower()==\"none\" else \"'\"+v+\"'\" for v in values ]\n",
    "assignment_list = \", \".join([\"`\"+k+\"`=\"+v for k,v in zip(row.keys(), values)])\n",
    "table_name = \"pytorch_cv_step_4\"\n",
    "pk_value = 1\n",
    "query = f\"UPDATE {table_name} SET {assignment_list} WHERE id = {pk_value}\"\n",
    "\n",
    "query\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:00:14.571203Z",
     "start_time": "2023-09-07T11:00:14.539768Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install sqlalchemy\n",
    "#!pip install mysql-connector-python\n",
    "#!pip install mysql-python < doesn't work, use pure-python version\n",
    "#!pip install pymysql\n",
    "#!pip install sqlalchemy_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['data_file', 'outer_epoch', 'inner_epoch', 'net__depth', 'net__width',\n       'net__activation', 'net__dropout', 'net__lr_rate', 'net__weight_decay',\n       'early_stopping', 'total_epochs', 'best_epoch', 'tuned_activation',\n       'tuned_dropout', 'tuned_lr_rate', 'tuned_weight_decay', 'classif.acc',\n       'classif.precision', 'classif.recall', 'classif.f1', 'classif.tn',\n       'classif.fp', 'classif.fn', 'classif.tp', 'guardian.rho_noco2',\n       'guardian.rlogit_noco2', 'guardian.rprob_noco2', 'guardian.rho_comp',\n       'guardian.rlogit_comp', 'guardian.rprob_comp', 'guardian.rho_pf',\n       'guardian.rlogit_pf', 'guardian.rprob_pf'],\n      dtype='object')"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "double_columns = ['tuned_dropout', 'tuned_lr_rate', 'tuned_weight_decay', 'classif.acc', 'classif.precision',\n",
    "       'classif.recall', 'classif.f1', 'classif.tn', 'classif.fp',\n",
    "       'classif.fn', 'classif.tp', 'guardian.rho_noco2',\n",
    "       'guardian.rlogit_noco2', 'guardian.rprob_noco2', 'guardian.rho_comp',\n",
    "       'guardian.rlogit_comp', 'guardian.rprob_comp', 'guardian.rho_pf',\n",
    "       'guardian.rlogit_pf', 'guardian.rprob_pf']\n",
    "double_dict = {k:pd.Series(dtype='double') for k in double_columns}\n",
    "int_dict = {k:pd.Series(dtype='int') for k in ['early_stopping', 'total_epochs', 'best_epoch']}\n",
    "string_dict = {k:pd.Series(dtype='string') for k in ['tuned_activation']}\n",
    "\n",
    "result_df = pd.DataFrame({**int_dict,**string_dict,**double_dict}) #, index=range(wundt_grid.shape[0])\n",
    "temp_df = pd.merge(outer_grid,result_df,how='outer',left_index=True,right_index=True)\n",
    "temp_df\n",
    "# can be used to create mysql table\n",
    "temp_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T11:00:15.377121Z",
     "start_time": "2023-09-07T11:00:15.361739Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:00:17.934565Z",
     "start_time": "2023-09-07T11:00:16.624303Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2023-09-07 13:00:17,913 - INFO - mysql_row_exists: SELECT COUNT(*) AS total FROM pytorch_cv_step_10 WHERE `data_file` = 'data_file' AND `outer_epoch` = 'outer_epoch' AND `inner_epoch` = 'inner_epoch' AND `net__depth` = 'net__depth' AND `net__width` = 'net__width' AND `net__activation` = 'net__activation' AND `net__dropout` = 'net__dropout' AND `net__lr_rate` = 'net__lr_rate' AND `net__weight_decay` = 'net__weight_decay'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  (84, 9)\n",
      "False\n",
      "(8, 34)\n",
      "after:  (82, 9)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "import sqlalchemy\n",
    "# timeouts: https://stackoverflow.com/questions/29755228/sqlalchemy-mysql-lost-connection-to-mysql-server-during-query\n",
    "engine = create_engine(\"mysql+pymysql://XXXXX:XXXXXXXXXXXX@127.0.0.1:33060/com1_ensiwiki-2020_agerank\", isolation_level=\"AUTOCOMMIT\", pool_recycle=3600, pool_pre_ping=True )\n",
    "# AUTOCOMMIT flag means statements are committed upon calling execute\n",
    "\n",
    "table_name = \"pytorch_cv_step_4\"\n",
    "# Create table\n",
    "if not sqlalchemy.inspect(engine).has_table(table_name):\n",
    "    with engine.connect() as con:\n",
    "        df_empty = temp_df.head(0)\n",
    "        df_empty.to_sql(con=con, name=table_name, if_exists='fail', index=False)\n",
    "        con.execute(text(f'ALTER TABLE {table_name} ADD id INT PRIMARY KEY AUTO_INCREMENT;'))\n",
    "        con.close()\n",
    "\n",
    "# Fetch the table data from the MySQL database\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "with engine.connect() as con:\n",
    "    sql_data = pd.read_sql_query(text(query), con) #.drop(\"id\", axis=1)\n",
    "\n",
    "print('before: ',outer_grid.shape)\n",
    "\n",
    "# Identify the rows in the dataframe that are not in the table\n",
    "# Manual approach using a string representation ...\n",
    "# kind-of a hack\n",
    "cols = outer_grid.columns.values.tolist()\n",
    "sqldata_strs = [' '.join(row.astype(str)) for idx, row in sql_data[cols].iterrows()]\n",
    "idxs = list()\n",
    "outer_grid.reset_index(inplace=True, drop=True)\n",
    "for idx, row in outer_grid.iterrows():\n",
    "    row_str = ' '.join(row.astype(str))\n",
    "    if row_str in sqldata_strs:\n",
    "        idxs.append(idx)\n",
    "#merged_data.loc[merged_data['_merge'] == \"right_only\", merged_data.columns!='_merge']\n",
    "outer_grid.drop(idxs, inplace=True)\n",
    "\n",
    "# From https://stackoverflow.com/questions/25104259/how-to-properly-escape-strings-when-manually-building-sql-queries-in-sqlalchemy\n",
    "def escape_like(l):\n",
    "    return l.replace('\\'','\\\"')\n",
    "\n",
    "# Helper function that check if row exists\n",
    "def mysql_row_exists(engine, pd_row, logger):\n",
    "    with engine.connect() as con:\n",
    "        values = [ \"NULL\" if v.lower() == \"nan\" or v.lower()==\"none\" else \"'\"+v+\"'\" for v in [escape_like(str(v)) for v in pd_row]] \n",
    "        # Create the WHERE statement\n",
    "        where_statement = \" AND \".join([f\"`{name}` = {value}\" for name, value in zip(pd_row.keys(), values)  if value!=\"NULL\" ])\n",
    "\n",
    "        # Create the WHERE query\n",
    "        where_query = f\"SELECT COUNT(*) AS total FROM {table_name} WHERE {where_statement}\"\n",
    "        #print(where_query)\n",
    "\n",
    "        # Execute the query and fetch the result\n",
    "        logger.info(\"mysql_row_exists: \" + where_query)\n",
    "        count_value = con.execute(text(where_query)).fetchone()[0]\n",
    "        con.close()\n",
    "        return count_value>0\n",
    "\n",
    "# row is a pd series\n",
    "# returns primary key auto increment value\n",
    "def mysql_insert_row(engine, pd_row, logger):\n",
    "    with engine.connect() as con:\n",
    "        # Create the INSERT statement with column names\n",
    "        column_names = \"(`\" + \"`,`\".join(pd_row.keys()) + \"`)\"\n",
    "        values = [ \"NULL\" if v.lower() == \"nan\" or v.lower()==\"none\" else \"'\"+v+\"'\" for v in [escape_like(str(v)) for v in pd_row]] \n",
    "        values = \"(\" + \",\".join(values) + \")\"\n",
    "        query = f\"INSERT INTO {table_name} {column_names} VALUES {values}\"\n",
    "        #print(query)\n",
    "        logger.info(\"mysql_insert_row: \" + query)\n",
    "        con.execute(text(query))\n",
    "\n",
    "        # Fetch the primary key value\n",
    "        pk_value = con.execute(text(\"SELECT LAST_INSERT_ID()\")).fetchone()[0]\n",
    "        con.close()\n",
    "        return pk_value\n",
    "\n",
    "# here, row is a dict!\n",
    "def mysql_update_row(engine, dict_row, pk_value, logger):\n",
    "    with engine.connect() as con:\n",
    "        values = [ \"NULL\" if v.lower() == \"nan\" or v.lower()==\"none\" else \"'\"+v+\"'\" for v in [escape_like(str(v)) for v in dict_row.values()]]\n",
    "        assignment_list = \", \".join([\"`\" + k +\"`=\" + v for k,v in zip(dict_row.keys(), values) if v!=\"NULL\"])\n",
    "        query = f\"UPDATE {table_name} SET {assignment_list} WHERE id = {pk_value}\"\n",
    "        #print(query)\n",
    "        logger.info(\"mysql_update_row: \" + query)\n",
    "        con.execute(text(query))\n",
    "        con.close()\n",
    "\n",
    "print(mysql_row_exists(engine, outer_grid.iloc[:1,:], logger))\n",
    "\n",
    "# outer_grid now contains the rows from expanded_grid that are not in the MySQL table\n",
    "print(sql_data.shape)\n",
    "#print(merged_data.shape)\n",
    "print('after: ', outer_grid.shape)\n",
    "#merged_data\n",
    "\n",
    "# From https://stackoverflow.com/questions/39582138/pandas-check-if-row-exist-in-another-dataframe-and-append-index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Net Builder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:01:19.704593Z",
     "start_time": "2023-09-07T11:01:19.657509Z"
    }
   },
   "outputs": [],
   "source": [
    "from com1py.net import LogitNet\n",
    "from com1py.torch_models import LogisticRegression as Model1\n",
    "from com1py.torch_models import Model2, Model3\n",
    "import torch\n",
    "\n",
    "def build_net(in_features, out_features, logger=logging.getLogger(), depth=1, lr_rate=.001, weight_decay=0,  width=25, activation=torch.nn.Tanh(),  dropout=.2, **kwargs):\n",
    "    assert len(kwargs)==0, \"unused params encountered while building Net\"+str(kwargs)\n",
    "\n",
    "    if activation==\"sigmoid\":\n",
    "        fun = torch.nn.Sigmoid()\n",
    "    elif activation==\"tanh\":\n",
    "        fun = torch.nn.Tanh()\n",
    "    elif activation==\"identity\":\n",
    "        fun = torch.nn.Identity()\n",
    "    elif activation==\"relu\":\n",
    "        fun = torch.nn.ReLU()\n",
    "    elif activation==\"softmax\":\n",
    "        fun = torch.nn.Softmax()\n",
    "    elif activation==None:\n",
    "        fun = None # only for depth=1 models\n",
    "    else:\n",
    "        log.error(\"Unknown activation function: %s\" % activation)\n",
    "\n",
    "    if depth==1:\n",
    "        model = Model1(in_features, out_features).to(device)\n",
    "    elif depth==2:\n",
    "        #print(in_features, out_features, width, fun)\n",
    "        model = Model2(in_features, out_features, int(width), fun, dropout).to(device)\n",
    "    elif depth==3:\n",
    "        model = Model3(in_features, out_features, int(width), fun, dropout).to(device)\n",
    "    else:\n",
    "        log.error(\"Unknown depth value encountered %d:\" % depth)\n",
    "        return\n",
    "\n",
    "    net = LogitNet(\n",
    "        model=model,\n",
    "        criterion=torch.nn.BCEWithLogitsLoss(),\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=lr_rate,weight_decay=weight_decay),\n",
    "        scheduler=None,\n",
    "        stopper=EarlyStopper(patience=PARAMS['stopper_patience'], min_delta=PARAMS['stopper_delta'], min_epochs=PARAMS['stopper_min_epochs'], logger=logger),\n",
    "        validate_every=PARAMS['validate_every'],\n",
    "        batch_size=PARAMS['batch_size'],\n",
    "        logging_level=PARAMS['net_logging_level'],\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:01:21.554856Z",
     "start_time": "2023-09-07T11:01:21.476409Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def preprocess(X_ts, y_ts, z, train_idxs, test_idxs):\n",
    "    # undersample the biggest class to get equal class sizes\n",
    "    Xt_train, yt_train, z_train, _ = random_undersample_t(X_ts[train_idxs], y_ts[train_idxs], z.iloc[train_idxs])\n",
    "    Xt_train, train_mean, train_std = standardize_t(Xt_train)\n",
    "    # no need to undersample the test observations\n",
    "    Xt_test, _, _ = standardize_t(X_ts[test_idxs,:]) \n",
    "    yt_test = y_ts[test_idxs]\n",
    "\n",
    "    return Xt_train, yt_train, Xt_test, yt_test, z_train, train_mean, train_std\n",
    "\n",
    "def train_net(Xt_train, yt_train, Xt_test, yt_test, net_params, epoch, logger):\n",
    "    log.info(\"Train net params: %s\" % net_params)\n",
    "    net = build_net(Xt_train.shape[1], 1, **net_params, logger=logger)\n",
    "\n",
    "    if not yt_test is None:\n",
    "        yt_test = yt_test.float()\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, test_loss = net.train(epoch, Xt_train, yt_train.float(), Xt_test, yt_test)\n",
    "    log.info(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return net, train_loss, test_loss\n",
    "\n",
    "# dict_b replaces dict_a\n",
    "def extract_net_params(dict_a, dict_b):\n",
    "    all_params = {**dict_a, **dict_b}\n",
    "    net_params = {k[5:]: v for k, v in all_params.items() if k.startswith('net__')}\n",
    "    return net_params, all_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T11:01:25.778615Z",
     "start_time": "2023-09-07T11:01:25.747251Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import FloatTensor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.base import clone\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def inner_loop(outer_row, X_ts, y_ts, z, log):\n",
    "    log.info(\"Inner loop cross-validation\")\n",
    "\n",
    "    # extract nested params\n",
    "    # when using iterrow, the column names are in the index of the row-series\n",
    "    cols = [c for c in outer_row.index if isinstance(outer_row[c], list)]\n",
    "    inner_grid = expand_grid(outer_row[cols].to_dict())\n",
    "    #outer_params = outer_row[ outer_row.index.difference(cols) ]\n",
    "    #print(\"inner_grid\", inner_grid) # key-value series\n",
    "    #print(\"outer_params\", outer_params)\n",
    "    inner_results = list()\n",
    "    if inner_grid.shape[0] < 2:\n",
    "        log.info(\"Skipping inner loop cause there is only %d rows to test\" % inner_grid.shape[0])\n",
    "        return outer_row, outer_row, pd.DataFrame() # no params to optimize in an inner loop\n",
    "\n",
    "    # loop params\n",
    "    for inner_index, inner_row in inner_grid.iterrows():\n",
    "        log.info(\"\\n\\n\\n\\nInner loop row index: %d/%d\" % (inner_index,inner_grid.shape[0]))\n",
    "        log.info(\"Inner loop params: %s\" % inner_row)\n",
    "\n",
    "        # Note: GroupKFold does not shuffle! The outer loop already shuffles the data. But still... strange.\n",
    "        cv = GroupKFold(n_splits=5)\n",
    "\n",
    "        y_pred = np.empty(y_ts.shape)\n",
    "        #y_logits = np.empty([y_ts.shape[0],1])\n",
    "        yt_logits = torch.empty([y_ts.shape[0],1], requires_grad=False).to(device)\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for train_idxs, test_idxs in cv.split(X_ts, y_ts, groups=z):\n",
    "            # Preprocess data\n",
    "            Xt_train, yt_train, Xt_test, yt_test, _, _, _ = preprocess(X_ts, y_ts, z, train_idxs, test_idxs)\n",
    "\n",
    "            # Extract params\n",
    "            net_params, params = extract_net_params(outer_row.to_dict(), inner_row.to_dict())\n",
    "\n",
    "            # Train neural network\n",
    "            net, train_loss, test_loss = train_net(Xt_train, yt_train, Xt_test, yt_test, net_params, params['inner_epoch'], log)\n",
    "\n",
    "            # train/test losses plot\n",
    "            #net.plot_losses().show()\n",
    "\n",
    "            # predict_proba \"will return whatever it is that the module’s forward() method returns, cast to a numpy.ndarray.\"\n",
    "\n",
    "            logit_ts, prob_ts, pred_ts = net.predict(Xt_test)\n",
    "            pred = pred_ts.squeeze(1).detach().cpu().numpy()\n",
    "            # two columns, one for each class\n",
    "            # calculate test losses\n",
    "            #print(pred.shape)\n",
    "\n",
    "            #m = metrics(yt_test.detach().cpu(), pred)\n",
    "            #log.info(\"Cross-validation metrics: \", m)\n",
    "\n",
    "            total_train_loss = total_train_loss + train_loss\n",
    "            y_pred[test_idxs] = pred\n",
    "            yt_logits[test_idxs] = logit_ts\n",
    "\n",
    "        untested_pred = np.count_nonzero(np.isnan(y_pred))\n",
    "        untested_logits = torch.count_nonzero(torch.isnan(yt_logits))\n",
    "        assert untested_pred==0 and untested_logits==0, \"not all observations were used as test data\"\n",
    "\n",
    "        #print(yt_logits.shape)\n",
    "        #print(y_ts.shape)\n",
    "        #print(type(y_ts))\n",
    "        m = metrics(y_ts.detach().cpu(), y_pred)\n",
    "        #print(m)\n",
    "        test_loss = criterion(yt_logits, y_ts.float().view(-1, 1))\n",
    "        #print(\"test loss\", test_loss.item())\n",
    "        #print(\"avg train loss\", total_train_loss / cv.get_n_splits())\n",
    "        m['avg_train_loss'] = total_train_loss / cv.get_n_splits()\n",
    "        m['test_loss'] = test_loss.item() #tensor with one value\n",
    "        inner_results.append(m)\n",
    "\n",
    "    inner_result = pd.DataFrame.from_dict(inner_results).join(inner_grid)\n",
    "    inner_result.sort_values(\"test_loss\", inplace=True)\n",
    "    log.info(\"cross-validation result: %s\" % inner_result)\n",
    "\n",
    "    winning_params = {k: v for k, v in inner_result.iloc[0,:].items() if k.startswith('net__')}\n",
    "    tune_params = {k: v for k, v in inner_result.iloc[0,:].items() if k in cols}\n",
    "    log.info(\"winning params: %s\" % winning_params)\n",
    "    log.info(\"tuned params: %s\" % tune_params)\n",
    "\n",
    "    return winning_params, tune_params, inner_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T11:52:18.907439Z",
     "start_time": "2023-08-15T11:25:54.094043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                               | 0/11 [00:00<?, ?it/s] 2023-08-15 13:25:54,104 - INFO - mysql_insert_row: INSERT INTO pytorch_cv (`data_file`,`outer_epoch`,`inner_epoch`,`net__depth`,`net__width`,`net__activation`,`net__dropout`,`net__lr_rate`,`net__weight_decay`) VALUES ('wundt_v16_8-model-ridge-na.parquet','1000','100','2','50.0','[\"sigmoid\", \"identity\", \"tanh\", \"relu\", \"softmax\"]','[0, 0.1, 0.2, 0.3, 0.4]','[1e-05, 0.0001, 0.001, 0.01, 0.1]','[0.0001, 0.001, 0.01, 0.1]')\n",
      "ID 68:   9%|██████▉                                                                     | 1/11 [26:24<4:24:07, 1584.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 105\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m counter\n\u001B[1;32m    102\u001B[0m \u001B[38;5;66;03m#results = \\\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m#only_1 = outer_grid.loc[outer_grid['net__depth']==1,]\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m#count = outer_loop(only_1)\u001B[39;00m\n\u001B[0;32m--> 105\u001B[0m count \u001B[38;5;241m=\u001B[39m \u001B[43mouter_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mouter_grid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m count\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m1\u001B[39m: \u001B[38;5;66;03m# means there were fewer than 10 iteration left\u001B[39;00m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[50], line 43\u001B[0m, in \u001B[0;36mouter_loop\u001B[0;34m(param_grid)\u001B[0m\n\u001B[1;32m     40\u001B[0m train_xt, train_yt, test_xt, test_yt, train_z \u001B[38;5;241m=\u001B[39m preprocess(X_ts, y_ts, z, train_idxs, test_idxs)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Tune model\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m winning_params, tune_params, tune_result \u001B[38;5;241m=\u001B[39m \u001B[43minner_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrid_row\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_xt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_yt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_z\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Re-train with winning params\u001B[39;00m\n\u001B[1;32m     46\u001B[0m net_params, params \u001B[38;5;241m=\u001B[39m extract_net_params(grid_row\u001B[38;5;241m.\u001B[39mto_dict(), tune_params)\n",
      "Cell \u001B[0;32mIn[40], line 51\u001B[0m, in \u001B[0;36minner_loop\u001B[0;34m(outer_row, X_ts, y_ts, z, log)\u001B[0m\n\u001B[1;32m     48\u001B[0m net_params, params \u001B[38;5;241m=\u001B[39m extract_net_params(outer_row\u001B[38;5;241m.\u001B[39mto_dict(), inner_row\u001B[38;5;241m.\u001B[39mto_dict())\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# Train neural network\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m net, train_loss, test_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXt_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myt_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXt_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myt_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minner_epoch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# train/test losses plot\u001B[39;00m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m#net.plot_losses().show()\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# predict_proba \"will return whatever it is that the module’s forward() method returns, cast to a numpy.ndarray.\"\u001B[39;00m\n\u001B[1;32m     58\u001B[0m logit_ts, prob_ts, pred_ts \u001B[38;5;241m=\u001B[39m net\u001B[38;5;241m.\u001B[39mpredict(Xt_test)\n",
      "Cell \u001B[0;32mIn[39], line 19\u001B[0m, in \u001B[0;36mtrain_net\u001B[0;34m(Xt_train, yt_train, Xt_test, yt_test, net_params, epoch, logger)\u001B[0m\n\u001B[1;32m     16\u001B[0m     yt_test \u001B[38;5;241m=\u001B[39m yt_test\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     18\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 19\u001B[0m train_loss, test_loss \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXt_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myt_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXt_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myt_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m log\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--- \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m seconds ---\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time))\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m net, train_loss, test_loss\n",
      "File \u001B[0;32m~/farm/src/com1-py/com1py/net.py:68\u001B[0m, in \u001B[0;36mLogitNet.train\u001B[0;34m(self, num_epochs, Xt_train, yt_train, Xt_val, yt_val)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m---> 68\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;66;03m# Save losses\u001B[39;00m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n",
      "File \u001B[0;32m~/farm/src/com1-py/com1py/net.py:35\u001B[0m, in \u001B[0;36mLogitNet.train_one_epoch\u001B[0;34m(self, train_loader)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 35\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Accumulate losses\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/bin/anaconda3/envs/jupyter-tensorflow/lib/python3.8/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/bin/anaconda3/envs/jupyter-tensorflow/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from helpers.load_parquet_data import load_parquet_data\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def outer_loop(param_grid):\n",
    "    # shuffle grid\n",
    "    param_grid = param_grid.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    counter = 0\n",
    "    with tqdm(total=param_grid.shape[0]) as pbar:\n",
    "        for grid_index, grid_row in param_grid.iterrows():\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Check if row exists, if not insert and get primary key value.\n",
    "            if mysql_row_exists(engine, grid_row, logger):\n",
    "                continue # stops this iteration, continues with rest\n",
    "            pk_value = mysql_insert_row(engine, grid_row, logger)\n",
    "            pbar.set_description(\"ID %d\" % pk_value)\n",
    "\n",
    "            log = new_log(f\"row{pk_value}\", f\"row{pk_value}.txt\")\n",
    "            #log = logger\n",
    "            log.info(\"\\n\\n\\n\\nOuter loop row index: %d/%d\" % (grid_index,param_grid.shape[0]))\n",
    "            log.info(\"Outer loop params: %s\" % grid_row)\n",
    "\n",
    "            # Load data\n",
    "            X, y, z, X_guardian, y_guardian,features = load_parquet_data(grid_row['data_file'])\n",
    "            X_ts = torch.from_numpy(X.to_numpy(np.float32)).to(device)\n",
    "            y_ts = torch.from_numpy(y.to_numpy(np.int64)).to(device)\n",
    "            X_guardian_ts = torch.from_numpy(X_guardian.to_numpy(np.float32)).to(device)\n",
    "            log.info(\"Loaded X: %s\" % str(X_ts.shape))\n",
    "\n",
    "            # train-test split\n",
    "            outer_cv = GroupShuffleSplit(n_splits=1, random_state=rng, test_size=None, train_size=0.8)\n",
    "\n",
    "            # Manual loop\n",
    "            for train_idxs, test_idxs in outer_cv.split(X, y, groups=z):\n",
    "                # Preprocess data\n",
    "                train_xt, train_yt, test_xt, test_yt, train_z, train_mean, train_std = preprocess(X_ts, y_ts, z, train_idxs, test_idxs)\n",
    "\n",
    "                # Tune model\n",
    "                winning_params, tune_params, tune_result = inner_loop(grid_row, train_xt, train_yt, train_z, log)\n",
    "\n",
    "                # Re-train with winning params\n",
    "                net_params, params = extract_net_params(grid_row.to_dict(), tune_params)\n",
    "                log.info(\"final net params: %s\" % net_params)\n",
    "                net, train_loss, test_loss = train_net(train_xt, train_yt, test_xt, test_yt, net_params, params['outer_epoch'], log)\n",
    "                plt = net.plot_losses()\n",
    "                if PARAMS['savefig']:\n",
    "                    plt.savefig(\"%s%d.png\" % (output_dir,pk_value) )\n",
    "                    plt.close()\n",
    "                #plt.show()\n",
    "                total_epochs = len(net.train_losses) # number of epochs done\n",
    "                early_stopping = net.stop_at_\n",
    "                best_epoch = net.best_epoch_\n",
    "\n",
    "                # Evaluate model performance using tuned parameters\n",
    "                logit_ts, prob_ts, pred_ts = net.predict(test_xt)\n",
    "                pred = pred_ts.detach().squeeze(1).cpu().numpy()\n",
    "                # two columns, one for each class\n",
    "                # calculate test losses\n",
    "                #print(pred.shape)\n",
    "\n",
    "                m = metrics(test_yt.detach().cpu(), pred)\n",
    "                log.info(\"train-test result: %s\" % m)\n",
    "\n",
    "            # Re-fit to whole data set (unless there was an early stop)\n",
    "            if net.stop_at_ is None:\n",
    "                # only if previous train didn't terminate early\n",
    "                # preprocess full data set - no test observations\n",
    "                train_xt, train_yt, _, _, train_z, train_mean, train_std = preprocess(X_ts, y_ts, z, np.arange(len(X_ts)), np.empty(0))\n",
    "                # re-train on full data set\n",
    "                net, train_loss, _ = train_net(X_ts, y_ts, None, None, net_params, params['outer_epoch'], log)\n",
    "                log.info(\"Reffited with train loss: %f\" % train_loss)\n",
    "            else:\n",
    "                log.info(\"Skipped re-fitting because earlier fit stopped early. Re-using earlier fit for target predictions.\")\n",
    "\n",
    "            # Evaluate on The Guardian\n",
    "            xt_guardian = standardize_t(X_guardian_ts, train_mean, train_std)\n",
    "            target_eval = guardian_eval(net, xt_guardian, y_guardian, log)\n",
    "            log.info(\"Target results: %s\" % target_eval)\n",
    "\n",
    "            tuned_params = {(\"tuned_\"+k[5:]): v for k, v in tune_params.items() }\n",
    "            row = {**grid_row.to_dict(), **tuned_params, **m, **target_eval}\n",
    "            row['total_epochs'] = total_epochs\n",
    "            row['best_epoch'] = best_epoch\n",
    "            row['early_stopping'] = early_stopping\n",
    "            mysql_update_row(engine, row, pk_value, log)\n",
    "\n",
    "            # we seem to be having a mem leak on the cpu ... so something stays glued into mem...\n",
    "            log.handlers.clear()\n",
    "            del X,y,z,X_guardian, y_guardian,features,X_ts,y_ts,X_guardian_ts, net,row,target_eval,train_loss,params,m,total_epochs,best_epoch,early_stopping,net_params,log,pred,logit_ts,prob_ts,pred_ts,plt,train_xt, train_yt, test_xt, test_yt, train_z, test_loss, tune_params, tune_result, train_idxs, test_idxs, outer_cv, pk_value,grid_index, grid_row\n",
    "            gc.collect()\n",
    "\n",
    "            # There's a mem leak ... stop after training 10 models\n",
    "            counter+=1\n",
    "            if counter>=1:\n",
    "                break\n",
    "    return counter\n",
    "\n",
    "count = outer_loop(outer_grid)\n",
    "\n",
    "if count<1: # means there were fewer than 10 iteration left\n",
    "    print(\"\\n\\nDone\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
